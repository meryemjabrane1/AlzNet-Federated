{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V5z3mYFsI3T"
      },
      "outputs": [],
      "source": [
        "#installing packages\n",
        "!pip install -q flwr[simulation] torch torchvision matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPgEwi-61v1d"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "\n",
        "import flwr as fl\n",
        "from flwr.common import Metrics\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
        "#print(f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vKXfxB42T52"
      },
      "outputs": [],
      "source": [
        "CLASSES = (\n",
        "    \"MildDemented\",\n",
        "    \"ModerateDemented\",\n",
        "    \"NonDemented\",\n",
        "    \"VeryMildDemented\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSvHygkG3V-f"
      },
      "outputs": [],
      "source": [
        "NUM_CLIENTS = 3\n",
        "BATCH_SIZE = 64\n",
        "NBR_CLASSES = 4\n",
        "epochs = 5\n",
        "num_rounds=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wexj_PAnnbO"
      },
      "outputs": [],
      "source": [
        "# Defining Custom dataset class (still gotta know if its necessary or not)\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = os.listdir(self.root_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_path = os.path.join(self.root_dir, self.image_paths[idx])\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrzaxUFFq4C_"
      },
      "outputs": [],
      "source": [
        "\n",
        "!wget -O AugmentedAlzheimerDataset.zip https://dl.dropboxusercontent.com/s/g933yjln90vhrzk/AugmentedAlzheimerDataset.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aSLhl_frTMy"
      },
      "outputs": [],
      "source": [
        "!unzip AugmentedAlzheimerDataset.zip -d destination_folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0yy_ZBx8RpC"
      },
      "outputs": [],
      "source": [
        "def load_datasets():\n",
        "    import os\n",
        "\n",
        "    # Download and transform (train and test)\n",
        "    transform = transforms.Compose(\n",
        "            [transforms.ToTensor(),transforms.Resize((224,224)), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "        )\n",
        "\n",
        "    root_dir = \"/content/destination_folder/AugmentedAlzheimerDataset\"\n",
        "    print(\"Listing root directory contents:\")\n",
        "    print(os.listdir(root_dir))\n",
        "\n",
        "    for class_name in ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']:\n",
        "        class_dir = os.path.join(root_dir, class_name)\n",
        "        print(f\"Listing contents of {class_name} directory:\")\n",
        "        print(os.listdir(class_dir))\n",
        "\n",
        "    # Load Dataset\n",
        "    dataset = datasets.ImageFolder(root=root_dir, transform=transform)\n",
        "\n",
        "    # Define Training and Testing Sets\n",
        "    trainset_size=int(0.9*len(dataset))\n",
        "    testset_size= len(dataset)-trainset_size\n",
        "    train_dataset, test_dataset = random_split(dataset,[trainset_size,testset_size])\n",
        "\n",
        "    # Split training set into 3 partitions to simulate the individual dataset\n",
        "    partition_size = len(train_dataset) // NUM_CLIENTS\n",
        "    lengths = [partition_size] * NUM_CLIENTS\n",
        "    clientdatasets = random_split(train_dataset, lengths, torch.Generator().manual_seed(42))\n",
        "\n",
        "    # Split each partition into train/val and create DataLoader\n",
        "    trainloaders = []\n",
        "    valloaders = []\n",
        "    for ds in clientdatasets:\n",
        "        len_val = len(ds) // 10  # 10 % validation set\n",
        "        len_train = len(ds) - len_val\n",
        "        lengths = [len_train, len_val]\n",
        "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
        "        trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))\n",
        "        valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))\n",
        "    testloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    return trainloaders, valloaders, testloader\n",
        "\n",
        "trainloaders, valloaders, testloader = load_datasets()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX30YdYr3Z3h"
      },
      "outputs": [],
      "source": [
        "images, labels = next(iter(trainloaders[0]))\n",
        "\n",
        "# Reshape and convert images to a NumPy array\n",
        "# matplotlib requires images with the shape (height, width, 3)\n",
        "images = images.permute(0, 2, 3, 1).numpy()\n",
        "# Denormalize\n",
        "images = images / 2 + 0.5\n",
        "\n",
        "# Create a figure and a grid of subplots\n",
        "fig, axs = plt.subplots(4, 8, figsize=(12, 6))\n",
        "\n",
        "# Loop over the images and plot them\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.imshow(images[i])\n",
        "    ax.set_title(CLASSES[labels[i]])\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Show the plot\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvCoZlfIfeBD"
      },
      "outputs": [],
      "source": [
        "#define model architecture\n",
        "model = models.densenet121(pretrained=True)\n",
        "#model = models.densenet121(progress=True)\n",
        "model.classifier = nn.Linear(1024, NBR_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6zktwXOzWdA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(model, trainloader, epochs: int, parameters: List[np.ndarray] = None):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    model_parameters = [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
        "\n",
        "    if parameters is not None:\n",
        "        for param in parameters:\n",
        "            if param.size == 0:\n",
        "                print(\"Empty numpy array found!\")\n",
        "        params_dict = zip(model.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "        model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        correct, total, epoch_loss = 0, 0, 0.0\n",
        "        for images, labels in trainloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(model(images), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Metrics\n",
        "            epoch_loss += loss\n",
        "            total += labels.size(0)\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "        epoch_loss /= len(trainloader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
        "\n",
        "def test(model, testloader):\n",
        "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    loss /= len(testloader.dataset)\n",
        "    accuracy = correct / total\n",
        "    return loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlfVgmX03gjl"
      },
      "outputs": [],
      "source": [
        "trainloader = trainloaders[0]\n",
        "valloader = valloaders[0]\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(model, trainloader, 1)\n",
        "    loss, accuracy = test(model, valloader)\n",
        "    print(f\"Epoch {epoch+1}: validation loss {loss}, accuracy {accuracy}\")\n",
        "\n",
        "loss, accuracy = test(model, testloader)\n",
        "print(f\"Final test set performance:\\n\\tloss {loss}\\n\\taccuracy {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfaVOpnuu7HZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, cid, model, trainloader, valloader):\n",
        "        self.cid = cid\n",
        "        self.model = model\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        print(f\"[Client {self.cid}] get_parameters\")\n",
        "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
        "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "        self.model.load_state_dict(state_dict, strict=True)\n",
        "        train(self.model, self.trainloader, epochs=1)\n",
        "        return self.get_parameters(config), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
        "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "        self.model.load_state_dict(state_dict, strict=True)\n",
        "        loss, accuracy = test(self.model, self.valloader)\n",
        "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
        "\n",
        "def client_fn(cid) -> FlowerClient:\n",
        "    model_copy = models.densenet121(pretrained=True)\n",
        "    model_copy.classifier = nn.Linear(1024, NBR_CLASSES)\n",
        "    model_copy.to(device)\n",
        "    trainloader = trainloaders[int(cid)]\n",
        "    valloader = valloaders[int(cid)]\n",
        "    return FlowerClient(cid, model_copy, trainloader, valloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EiQFHk4_FFr"
      },
      "outputs": [],
      "source": [
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    # Multiply accuracy of each client by number of examples used\n",
        "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "\n",
        "    # Aggregate and return custom metric (weighted average)\n",
        "    return {\"accuracy\": sum(accuracies) / sum(examples)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq8Y9dMMmpIe"
      },
      "outputs": [],
      "source": [
        "# The `evaluate` function will be by Flower called after every round\n",
        "\"\"\"def evaluate(\n",
        "    server_round: int,\n",
        "    parameters: fl.common.NDArrays,\n",
        "    config: Dict[str, fl.common.Scalar],\n",
        "    model,\n",
        ") -> Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:\n",
        "    model = model.to(device)\n",
        "    valloader = valloaders[0]\n",
        "    set_parameters(model, parameters)  # Update model with the latest parameters\n",
        "    loss, accuracy = test(model, valloader)\n",
        "    print(f\"Server-side evaluation loss {loss} / accuracy {accuracy}\")\n",
        "    return loss, {\"accuracy\": accuracy}\"\"\"\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    server_round: int,\n",
        "    parameters: fl.common.NDArrays,\n",
        "    config: Dict[str, fl.common.Scalar],\n",
        "    model,\n",
        ") -> Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:\n",
        "    print (f\"-------/{server_round}/--------{config}\")\n",
        "    model = model.to(device)\n",
        "    valloader = valloaders[0]\n",
        "\n",
        "    # Set model parameters directly without calling set_parameters function\n",
        "    for param in parameters:\n",
        "        if param.size == 0:\n",
        "            print(\"Empty numpy array found!\")\n",
        "\n",
        "    params_dict = zip(model.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "    model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    loss, accuracy = test(model, valloader)\n",
        "    print(f\"Server-side evaluation loss {loss} / accuracy {accuracy}\")\n",
        "    torch.cuda.empty_cache()\n",
        "    return loss, {\"accuracy\": accuracy}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMQWBm5klGGD"
      },
      "outputs": [],
      "source": [
        "\n",
        "def fit_config(server_round: int):\n",
        "    \"\"\"Return training configuration dict for each round.\n",
        "\n",
        "    Perform two rounds of training with one local epoch, increase to two local\n",
        "    epochs afterwards.\n",
        "    \"\"\"\n",
        "    config = {\n",
        "        \"server_round\": server_round,  # The current round of federated learning\n",
        "        \"local_epochs\": 1 if server_round < 2 else 2,  #\n",
        "    }\n",
        "    return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irXVPzOlFbZn",
        "outputId": "a91f4a10-5aeb-49cc-c4d9-b4999cc10dd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO flwr 2023-04-24 18:20:02,055 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=2, round_timeout=None)\n",
            "INFO:flwr:Starting Flower simulation, config: ServerConfig(num_rounds=2, round_timeout=None)\n",
            "2023-04-24 18:20:03,727\tINFO worker.py:1553 -- Started a local Ray instance.\n",
            "INFO flwr 2023-04-24 18:20:05,049 | app.py:180 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 26672431104.0, 'memory': 53344862208.0, 'accelerator_type:A100': 1.0, 'node:172.28.0.12': 1.0, 'CPU': 12.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'object_store_memory': 26672431104.0, 'memory': 53344862208.0, 'accelerator_type:A100': 1.0, 'node:172.28.0.12': 1.0, 'CPU': 12.0}\n",
            "INFO flwr 2023-04-24 18:20:05,053 | server.py:86 | Initializing global parameters\n",
            "INFO:flwr:Initializing global parameters\n",
            "INFO flwr 2023-04-24 18:20:05,055 | server.py:269 | Using initial parameters provided by strategy\n",
            "INFO:flwr:Using initial parameters provided by strategy\n",
            "INFO flwr 2023-04-24 18:20:05,058 | server.py:88 | Evaluating initial parameters\n",
            "INFO:flwr:Evaluating initial parameters\n",
            "INFO flwr 2023-04-24 18:20:05,060 | server.py:101 | FL starting\n",
            "INFO:flwr:FL starting\n",
            "DEBUG flwr 2023-04-24 18:20:05,064 | server.py:218 | fit_round 1: strategy sampled 3 clients (out of 3)\n",
            "DEBUG:flwr:fit_round 1: strategy sampled 3 clients (out of 3)\n",
            "\u001b[2m\u001b[36m(pid=21402)\u001b[0m 2023-04-24 18:20:06.829290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=21402)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=21402)\u001b[0m   warnings.warn(\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=21402)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=21402)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=21402)\u001b[0m [Client 1] fit, config: {'server_round': 1, 'local_epochs': 1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=21402)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=21402)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=21402)\u001b[0m Epoch 1: train loss 0.01770535483956337, accuracy 0.8913469921534438\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=21402)\u001b[0m [Client 1] get_parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=21900)\u001b[0m 2023-04-24 18:21:37.319534: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=21900)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=21900)\u001b[0m   warnings.warn(\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=21900)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=21900)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=21900)\u001b[0m [Client 2] fit, config: {'server_round': 1, 'local_epochs': 1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=21900)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=21900)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=21900)\u001b[0m Epoch 1: train loss 0.018159398809075356, accuracy 0.8866608544027899\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=21900)\u001b[0m [Client 2] get_parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=22374)\u001b[0m 2023-04-24 18:23:07.955006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=22374)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=22374)\u001b[0m   warnings.warn(\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=22374)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=22374)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=22374)\u001b[0m [Client 0] fit, config: {'server_round': 1, 'local_epochs': 1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=22374)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=22374)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=22374)\u001b[0m Epoch 1: train loss 0.013332301750779152, accuracy 0.922079337401918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-04-24 18:24:37,133 | server.py:232 | fit_round 1 received 3 results and 0 failures\n",
            "DEBUG:flwr:fit_round 1 received 3 results and 0 failures\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=22374)\u001b[0m [Client 0] get_parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING flwr 2023-04-24 18:24:37,733 | fedavg.py:243 | No fit_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No fit_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-04-24 18:24:37,737 | server.py:168 | evaluate_round 1: strategy sampled 3 clients (out of 3)\n",
            "DEBUG:flwr:evaluate_round 1: strategy sampled 3 clients (out of 3)\n",
            "\u001b[2m\u001b[36m(pid=22854)\u001b[0m 2023-04-24 18:24:39.535231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=22854)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=22854)\u001b[0m   warnings.warn(\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=22854)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=22854)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=22854)\u001b[0m [Client 2] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=22854)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=22854)\u001b[0m   warnings.warn(\n",
            "\u001b[2m\u001b[36m(pid=22972)\u001b[0m 2023-04-24 18:24:49.917051: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=22972)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=22972)\u001b[0m   warnings.warn(\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=22972)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=22972)\u001b[0m   warnings.warn(msg)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=22972)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=22972)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=22972)\u001b[0m [Client 0] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=23086)\u001b[0m 2023-04-24 18:25:00.384251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=23086)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=23086)\u001b[0m   warnings.warn(\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=23086)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=23086)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=23086)\u001b[0m [Client 1] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=23086)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=23086)\u001b[0m   warnings.warn(\n",
            "DEBUG flwr 2023-04-24 18:25:09,150 | server.py:182 | evaluate_round 1 received 3 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 1 received 3 results and 0 failures\n",
            "WARNING flwr 2023-04-24 18:25:09,152 | fedavg.py:274 | No evaluate_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No evaluate_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-04-24 18:25:09,154 | server.py:218 | fit_round 2: strategy sampled 3 clients (out of 3)\n",
            "DEBUG:flwr:fit_round 2: strategy sampled 3 clients (out of 3)\n",
            "\u001b[2m\u001b[36m(pid=23201)\u001b[0m 2023-04-24 18:25:10.842018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23201)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23201)\u001b[0m   warnings.warn(\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23201)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23201)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=23201)\u001b[0m [Client 1] fit, config: {'server_round': 2, 'local_epochs': 2}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=23201)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23201)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=23201)\u001b[0m Epoch 1: train loss 0.014440739527344704, accuracy 0.9118352223190933\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23201)\u001b[0m [Client 1] get_parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=23676)\u001b[0m 2023-04-24 18:26:41.712871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23676)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23676)\u001b[0m   warnings.warn(\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23676)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23676)\u001b[0m   warnings.warn(msg)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23676)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23676)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=23676)\u001b[0m [Client 0] fit, config: {'server_round': 2, 'local_epochs': 2}\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23676)\u001b[0m Epoch 1: train loss 0.01102272979915142, accuracy 0.9353748910200523\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23676)\u001b[0m [Client 0] get_parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=24149)\u001b[0m 2023-04-24 18:28:11.953194: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=24149)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=24149)\u001b[0m   warnings.warn(\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=24149)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=24149)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=24149)\u001b[0m [Client 2] fit, config: {'server_round': 2, 'local_epochs': 2}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=24149)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=24149)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=24149)\u001b[0m Epoch 1: train loss 0.01459597609937191, accuracy 0.9102005231037489\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-04-24 18:29:40,880 | server.py:232 | fit_round 2 received 3 results and 0 failures\n",
            "DEBUG:flwr:fit_round 2 received 3 results and 0 failures\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=24149)\u001b[0m [Client 2] get_parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-04-24 18:29:41,414 | server.py:168 | evaluate_round 2: strategy sampled 3 clients (out of 3)\n",
            "DEBUG:flwr:evaluate_round 2: strategy sampled 3 clients (out of 3)\n",
            "\u001b[2m\u001b[36m(pid=24628)\u001b[0m 2023-04-24 18:29:43.214778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=24628)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=24628)\u001b[0m   warnings.warn(\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=24628)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=24628)\u001b[0m   warnings.warn(msg)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=24628)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=24628)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=24628)\u001b[0m [Client 0] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR flwr 2023-04-24 18:29:47,117 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=24628, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
            "    return maybe_call_evaluate(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/flwr/client/client.py\", line 205, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/flwr/client/app.py\", line 321, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "  File \"<ipython-input-11-fd86fd150091>\", line 25, in evaluate\n",
            "  File \"<ipython-input-9-336b4cc2d635>\", line 41, in test\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torchvision/models/densenet.py\", line 213, in forward\n",
            "    features = self.features(x)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/activation.py\", line 103, in forward\n",
            "    return F.relu(input, inplace=self.inplace)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\", line 1455, in relu\n",
            "    result = torch.relu_(input)\n",
            "RuntimeError: CUDA error: out of memory\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "ERROR:flwr:\u001b[36mray::launch_and_evaluate()\u001b[39m (pid=24628, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
            "    return maybe_call_evaluate(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/flwr/client/client.py\", line 205, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/flwr/client/app.py\", line 321, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "  File \"<ipython-input-11-fd86fd150091>\", line 25, in evaluate\n",
            "  File \"<ipython-input-9-336b4cc2d635>\", line 41, in test\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torchvision/models/densenet.py\", line 213, in forward\n",
            "    features = self.features(x)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/activation.py\", line 103, in forward\n",
            "    return F.relu(input, inplace=self.inplace)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\", line 1455, in relu\n",
            "    result = torch.relu_(input)\n",
            "RuntimeError: CUDA error: out of memory\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\u001b[2m\u001b[36m(pid=24720)\u001b[0m 2023-04-24 18:29:48.735958: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=24720)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=24720)\u001b[0m   warnings.warn(\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=24720)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=24720)\u001b[0m   warnings.warn(msg)\n",
            "ERROR flwr 2023-04-24 18:29:52,138 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=24720, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 159, in launch_and_evaluate\n",
            "    client: Client = _create_client(client_fn, cid)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 168, in _create_client\n",
            "    client_like: ClientLike = client_fn(cid)\n",
            "  File \"<ipython-input-11-fd86fd150091>\", line 31, in client_fn\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1145, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 820, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1143, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: out of memory\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "ERROR:flwr:\u001b[36mray::launch_and_evaluate()\u001b[39m (pid=24720, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 159, in launch_and_evaluate\n",
            "    client: Client = _create_client(client_fn, cid)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 168, in _create_client\n",
            "    client_like: ClientLike = client_fn(cid)\n",
            "  File \"<ipython-input-11-fd86fd150091>\", line 31, in client_fn\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1145, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 820, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1143, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: out of memory\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\u001b[2m\u001b[36m(pid=24809)\u001b[0m 2023-04-24 18:29:53.736314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=24809)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=24809)\u001b[0m   warnings.warn(\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=24809)\u001b[0m /usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=24809)\u001b[0m   warnings.warn(msg)\n",
            "ERROR flwr 2023-04-24 18:29:57,060 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=24809, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 159, in launch_and_evaluate\n",
            "    client: Client = _create_client(client_fn, cid)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 168, in _create_client\n",
            "    client_like: ClientLike = client_fn(cid)\n",
            "  File \"<ipython-input-11-fd86fd150091>\", line 31, in client_fn\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1145, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 820, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1143, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: out of memory\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "ERROR:flwr:\u001b[36mray::launch_and_evaluate()\u001b[39m (pid=24809, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 159, in launch_and_evaluate\n",
            "    client: Client = _create_client(client_fn, cid)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 168, in _create_client\n",
            "    client_like: ClientLike = client_fn(cid)\n",
            "  File \"<ipython-input-11-fd86fd150091>\", line 31, in client_fn\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1145, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 820, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1143, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: out of memory\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "DEBUG flwr 2023-04-24 18:29:57,064 | server.py:182 | evaluate_round 2 received 0 results and 3 failures\n",
            "DEBUG:flwr:evaluate_round 2 received 0 results and 3 failures\n",
            "INFO flwr 2023-04-24 18:29:57,067 | server.py:147 | FL finished in 592.0032685890001\n",
            "INFO:flwr:FL finished in 592.0032685890001\n",
            "INFO flwr 2023-04-24 18:29:57,070 | app.py:218 | app_fit: losses_distributed [(1, 0.00725825301910445)]\n",
            "INFO:flwr:app_fit: losses_distributed [(1, 0.00725825301910445)]\n",
            "INFO flwr 2023-04-24 18:29:57,073 | app.py:219 | app_fit: metrics_distributed_fit {}\n",
            "INFO:flwr:app_fit: metrics_distributed_fit {}\n",
            "INFO flwr 2023-04-24 18:29:57,075 | app.py:220 | app_fit: metrics_distributed {}\n",
            "INFO:flwr:app_fit: metrics_distributed {}\n",
            "INFO flwr 2023-04-24 18:29:57,081 | app.py:221 | app_fit: losses_centralized []\n",
            "INFO:flwr:app_fit: losses_centralized []\n",
            "INFO flwr 2023-04-24 18:29:57,083 | app.py:222 | app_fit: metrics_centralized {}\n",
            "INFO:flwr:app_fit: metrics_centralized {}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "History (loss, distributed):\n",
              "\tround 1: 0.00725825301910445"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create an instance of the model and get the parameters\n",
        "\n",
        "model_parameters = [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
        "# Pass parameters to the Strategy for server-side parameter initialization\n",
        "strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=1.0,\n",
        "    fraction_evaluate=1.0,\n",
        "    min_fit_clients=2,\n",
        "    min_evaluate_clients=2,\n",
        "    min_available_clients=NUM_CLIENTS,\n",
        "    initial_parameters=fl.common.ndarrays_to_parameters(model_parameters),\n",
        "    on_fit_config_fn=fit_config  # Pass the fit_config function\n",
        "    # evaluate_fn=evaluate(server_round=0, parameters=model_parameters, config=fit_config(0), model=model),\n",
        ")\n",
        "\n",
        "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
        "client_resources = None\n",
        "if device.type == \"cuda\":\n",
        "    client_resources = {\"num_gpus\": 1}\n",
        "\n",
        "# Start simulation\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=2),  # six rounds\n",
        "    strategy=strategy,\n",
        "    client_resources=client_resources,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
